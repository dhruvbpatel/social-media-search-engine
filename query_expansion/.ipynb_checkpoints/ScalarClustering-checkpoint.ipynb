{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa96b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "#import pysolr\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b1a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c867c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text(str): a string of text\n",
    "    Return:\n",
    "        tokens(list): a list of cleaned tokens\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    text = re.sub(r'[\\n]', ' ', text) # remove enters\n",
    "    text = re.sub(r'[,-]', ' ', text) # remove comma and dash\n",
    "    text = re.sub('[0-9]', '', text) # remove all numbers\n",
    "    text = re.sub(r'[^\\W\\w\\s]', '', text) # only keep A-Za-z_ and space\n",
    "    text = text.lower()\n",
    "    tkns = text.split()\n",
    "    # double check, remove empty tokens, stop words, and full numeric tokens\n",
    "    tokens = [token for token in tkns if token not in stop_words and token != '' and not token.isnumeric()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44732c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stem_map(vocab):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab(list): a list of vocabulary\n",
    "    Returns:\n",
    "        token_2_stem(dict): a map from token to its stem having structure {token:stem}\n",
    "        stem_2_tokens(dict): a map from stem to its corresponding tokens having structure:\n",
    "                             {stem:set(token_1, token_2, ...)}\n",
    "    \"\"\"\n",
    "    token_2_stem = {} # 1 to 1\n",
    "    stem_2_tokens = {} # 1 to n\n",
    "\n",
    "    for token in vocab:\n",
    "        stem = porter_stemmer.stem(token)\n",
    "        if stem not in stem_2_tokens:\n",
    "            stem_2_tokens[stem] = set()\n",
    "        stem_2_tokens[stem].add(token)\n",
    "        token_2_stem[token] = stem\n",
    "\n",
    "    return token_2_stem, stem_2_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b82a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalar_cluster(doc_tokens, token_2_stem, stem_2_tokens, query):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        doc_tokens(2-D list): tokens in each documents having structure:\n",
    "                              [[token_1, token_2, ...], [...], ...]\n",
    "        token_2_stem(dict): a map from token to its stem having structure {token:stem}\n",
    "        stem_2_tokens(dict): a map from stem to its corresponding tokens having structure:\n",
    "                             {stem:set(token_1, token_2, ...)}\n",
    "        query(list): a list of tokens from query\n",
    "        \n",
    "    Return:\n",
    "        query_expands(list): list of expand stem tokens ids for each token in the query\n",
    "    \"\"\"\n",
    "    # build map from stem to index\n",
    "    stems = stem_2_tokens.keys()\n",
    "    stems = list(sorted(stems))\n",
    "    stem_2_idx = {s:i for i, s in enumerate(stems)}\n",
    "\n",
    "    # print('Vocab:', token_2_stem.keys())\n",
    "    # print('Stems:', stem_2_idx.keys())\n",
    "\n",
    "    # frequency of stems in each document\n",
    "    f = np.zeros((len(doc_tokens), len(stems)), dtype=np.int)\n",
    "    for doc_id, tokens in enumerate(doc_tokens):\n",
    "        for token in tokens:\n",
    "            if token in token_2_stem:\n",
    "                stem = token_2_stem[token]\n",
    "                stem_idx = stem_2_idx[stem]\n",
    "                f[doc_id, stem_idx] += 1\n",
    "\n",
    "    # correlation matrix\n",
    "    c = np.dot(f.T, f) # (#_of_stems, #_of_stems)\n",
    "    c_diag = np.expand_dims(np.diag(c), axis=0) # extract all c_{u,u} shape: (1, #_of_stems)\n",
    "\n",
    "    # normalize correlation matrix\n",
    "    s = c / (c + c_diag + c_diag.T) # (#_of_stems, #_of_stems)\n",
    "    s_norm = np.linalg.norm(s, axis=1) # (#_of_stems,)\n",
    "\n",
    "    # expand query\n",
    "    query_expands_id = []\n",
    "    for token in query:\n",
    "        stem = token_2_stem[token]\n",
    "        stem_id = stem_2_idx[stem]\n",
    "\n",
    "        # calculate cosine simialrity for the token with all other stems\n",
    "        stem_vec = np.expand_dims(s[stem_id, :], axis=0)\n",
    "        stem_norm = np.linalg.norm(stem_vec)\n",
    "        s_stem = np.dot(stem_vec, s.T).squeeze() # (#_of_stems,)\n",
    "        s_stem = (s_stem / stem_norm) / s_norm # cosine similarity\n",
    "\n",
    "        # pick the top 3 stems for each query token\n",
    "        idx_sort = np.argsort(s_stem)[::-1] # sort decreasing by score\n",
    "        idx_sort = idx_sort[:2]\n",
    "        query_expands_id.extend(idx_sort.tolist())\n",
    "\n",
    "    # convert stem ids to stem\n",
    "    query_expands = []\n",
    "    for stem_idx in query_expands_id:\n",
    "        query_expands.append(stems[stem_idx])\n",
    "\n",
    "    return query_expands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc71ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_main(query, solr_results):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query(str): a text string of query\n",
    "        solr_results(list): result for the query from function 'get_results_from_solr'\n",
    "    Return:\n",
    "        query(str): a text string of expanded query\n",
    "    \"\"\"\n",
    "    # query = 'Michael Phelps'\n",
    "    # solr = pysolr.Solr('http://localhost:8983/solr/nutch/', always_commit=True, timeout=10)\n",
    "    # results = get_results_from_solr(query, solr)\n",
    "    vocab = set()\n",
    "    doc_tokens = []\n",
    "\n",
    "    # tokenize query and query results, then build vocabulary\n",
    "    if 'content:' == query[:8]:\n",
    "        query = query[8:]\n",
    "    query_text = query # keep original query text\n",
    "    query = tokenize_text(query)\n",
    "    vocab.update(query)\n",
    "    for result in tqdm(solr_results, desc='Preprocessing results'):\n",
    "        if 'content' not in result:\n",
    "            tokens = []\n",
    "        else:\n",
    "            tokens = tokenize_text(result['content'])\n",
    "        doc_tokens.append(tokens)\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    vocab = list(sorted(vocab))\n",
    "    token_2_stem, stem_2_tokens = make_stem_map(vocab)\n",
    "\n",
    "    # expand query\n",
    "    query_expands_stem = get_scalar_cluster(doc_tokens, token_2_stem, stem_2_tokens, query)\n",
    "    # convert from stem to tokens\n",
    "    query_expands = set()\n",
    "    for stem in query_expands_stem:\n",
    "        query_expands.update(list(stem_2_tokens[stem]))\n",
    "    # generate new query\n",
    "    for token in query:\n",
    "        query_expands.discard(token)\n",
    "    query.extend(list(query_expands))\n",
    "    query = ' '.join(query)\n",
    "\n",
    "    print('Expanded query:', query)\n",
    "    query = 'content:' + query\n",
    "\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1a719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('response-500.json', 'r', encoding=\"utf8\") as f:\n",
    "    # Load JSON data as a list of dictionaries\n",
    "    data = json.load(f)\n",
    "\n",
    "doc_list = data[\"response\"][\"docs\"]\n",
    "documents = []\n",
    "for doc in doc_list:\n",
    "    if \"content\" in doc:\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7098429",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"social\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f241532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing results: 100%|███████████████████████████████████████████████████████| 281/281 [00:00<00:00, 1992.99it/s]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_19400\\3577699429.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  f = np.zeros((len(doc_tokens), len(stems)), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query: social () socially\n",
      "content:social () socially\n"
     ]
    }
   ],
   "source": [
    "queryRes = scalar_main(query, documents)\n",
    "print(queryRes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
