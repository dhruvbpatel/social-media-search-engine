{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01aa161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "#import pysolr\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98dcbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c24eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text(str): a string of text\n",
    "\n",
    "    Return:\n",
    "        tokens(list): a list of cleaned tokens\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    text = re.sub(r'[\\n]', ' ', text) # remove enters\n",
    "    text = re.sub(r'[,-]', ' ', text) # remove comma and dash\n",
    "    text = re.sub('[0-9]', '', text) # remove all numbers\n",
    "    text = re.sub(r'[^\\W\\w\\s]', '', text) # only keep A-Za-z_ and space\n",
    "    text = text.lower()\n",
    "    tkns = text.split()\n",
    "    # double check, remove empty tokens, stop words, and full numeric tokens\n",
    "    tokens = [token for token in tkns if token not in stop_words and token != '' and not token.isnumeric()]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f058227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stem_map(vocab):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab(list): a list of vocabulary\n",
    "\n",
    "    Returns:\n",
    "        token_2_stem(dict): a map from token to its stem having structure {token:stem}\n",
    "        stem_2_tokens(dict): a map from stem to its corresponding tokens having structure:\n",
    "                             {stem:set(token_1, token_2, ...)}\n",
    "    \"\"\"\n",
    "    token_2_stem = {} # 1 to 1\n",
    "    stem_2_tokens = {} # 1 to n\n",
    "\n",
    "    for token in vocab:\n",
    "        stem = porter_stemmer.stem(token)\n",
    "        if stem not in stem_2_tokens:\n",
    "            stem_2_tokens[stem] = set()\n",
    "        stem_2_tokens[stem].add(token)\n",
    "        token_2_stem[token] = stem\n",
    "\n",
    "    return token_2_stem, stem_2_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb7b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_clusters(doc_tokens, token_2_stem, stem_2_tokens, query):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        doc_tokens(2-D list): tokens in each documents having structure:\n",
    "                              [[token_1, token_2, ...], [...], ...]\n",
    "        token_2_stem(dict): a map from token to its stem having structure {token:stem}\n",
    "        stem_2_tokens(dict): a map from stem to its corresponding tokens having structure:\n",
    "                             {stem:set(token_1, token_2, ...)}\n",
    "        query(list): a list of tokens from query\n",
    "        \n",
    "    Return:\n",
    "        query_expands(list): list of expand stem tokens ids for each token in the query\n",
    "    \"\"\"\n",
    "    # build map from stem to index\n",
    "    stems = stem_2_tokens.keys()\n",
    "    stems = list(sorted(stems))\n",
    "    stem_2_idx = {s:i for i, s in enumerate(stems)}\n",
    "\n",
    "    # print('Vocab:', token_2_stem.keys())\n",
    "    # print('Stems:', stem_2_idx.keys())\n",
    "\n",
    "    # count the number of variants for each stem\n",
    "    stem_len = [len(stem_2_tokens[s]) for s in stems]\n",
    "    stem_len = np.array(stem_len)\n",
    "\n",
    "    # build correlation matrix\n",
    "    c = np.zeros((len(stem_2_idx), len(stem_2_idx)), dtype=np.int)\n",
    "    for doc_id, tokens in enumerate(doc_tokens):\n",
    "        tokens_count = Counter(tokens)\n",
    "        # for each documents, count the difference between each pair of tokens\n",
    "        # and add them to their corresponding stems\n",
    "        for token_1, count_1 in tokens_count.items():\n",
    "            stem_1 = token_2_stem[token_1]\n",
    "            stem_1_id = stem_2_idx[stem_1]\n",
    "            for token_2, count_2 in tokens_count.items():\n",
    "                stem_2 = token_2_stem[token_2]\n",
    "                stem_2_id = stem_2_idx[stem_2]\n",
    "                if stem_1 == stem_2:\n",
    "                    continue\n",
    "                if count_1 != count_2:\n",
    "                    c[stem_1_id, stem_2_id] += 1. / abs(count_1 - count_2)\n",
    "\n",
    "    # normalize correlation matrix and pick the top 3 expansion tokens\n",
    "    query_expands_id = []\n",
    "    for token in query:\n",
    "        stem = token_2_stem[token]\n",
    "        stem_id = stem_2_idx[stem]\n",
    "\n",
    "        # normalize correlation matrix\n",
    "        s_stem = c[stem_id, :] / (stem_len[stem_id] * stem_len)\n",
    "\n",
    "        # pick the top 3 stems for each query token\n",
    "        s_stem = np.argsort(s_stem)[::-1] # sort decreasing by score\n",
    "        s_stem = s_stem[:2]\n",
    "        query_expands_id.extend(s_stem.tolist())\n",
    "\n",
    "    # convert stem ids to stem\n",
    "    query_expands = []\n",
    "    for stem_idx in query_expands_id:\n",
    "        query_expands.append(stems[stem_idx])\n",
    "\n",
    "    return query_expands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71f0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_cluster_main(query, solr_results):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query(str): a text string of query\n",
    "        solr_results(list): result for the query from function 'get_results_from_solr'\n",
    "\n",
    "    Return:\n",
    "        query(str): a text string of expanded query\n",
    "    \"\"\"\n",
    "    # query = 'olympic medal'\n",
    "    # solr = pysolr.Solr('http://localhost:8983/solr/nutch/', always_commit=True, timeout=10)\n",
    "    # results = get_results_from_solr(query, solr)\n",
    "    vocab = set()\n",
    "    doc_tokens = []\n",
    "\n",
    "    # tokenize query and query results, then build stem\n",
    "    if 'content:' == query[:8]:\n",
    "        query = query[8:]\n",
    "    query_text = query\n",
    "    query = tokenize_text(query)\n",
    "    vocab.update(query)\n",
    "    for result in tqdm(solr_results, desc='Preprocessing results'):\n",
    "        if 'content' not in result:\n",
    "            tokens = []\n",
    "        else:\n",
    "            tokens = tokenize_text(result['content'])\n",
    "        doc_tokens.append(tokens)\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    vocab = list(sorted(vocab))\n",
    "    token_2_stem, stem_2_tokens = make_stem_map(vocab)\n",
    "\n",
    "    # expand query\n",
    "    query_expands_stem = get_metric_clusters(doc_tokens, token_2_stem, stem_2_tokens, query)\n",
    "    # convert from stem to tokens\n",
    "    query_expands = set()\n",
    "    for stem in query_expands_stem:\n",
    "        query_expands.update(list(stem_2_tokens[stem]))\n",
    "    # generate new query\n",
    "    for token in query:\n",
    "        query_expands.discard(token)\n",
    "    query.extend(list(query_expands))\n",
    "    query = ' '.join(query)\n",
    "\n",
    "    print('Expanded query:', query)\n",
    "    query = 'content:' + query\n",
    "\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dca1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('solr_data_5000.json', 'r', encoding=\"utf8\") as f:\n",
    "    # Load JSON data as a list of dictionaries\n",
    "    data = json.load(f)\n",
    "\n",
    "doc_list = data[\"response\"][\"docs\"]\n",
    "documents = []\n",
    "for doc in doc_list:\n",
    "    if \"content\" in doc:\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e79fa1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"social\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c23536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing results: 100%|██████████| 4780/4780 [00:02<00:00, 2377.17it/s]\n",
      "C:\\Users\\sxg200022\\AppData\\Local\\Temp\\ipykernel_11048\\3290085069.py:27: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  c = np.zeros((len(stem_2_idx), len(stem_2_idx)), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query: social first health\n",
      "content:social first health\n"
     ]
    }
   ],
   "source": [
    "queryRes = metric_cluster_main(query, documents)\n",
    "print(queryRes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
